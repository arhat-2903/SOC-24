{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "053ea6a5-6146-4ca0-b05f-a42e14dd0f35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "053ea6a5-6146-4ca0-b05f-a42e14dd0f35",
        "outputId": "277c2032-14bb-4914-da86-2691d1ff8e5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e749374-7b8b-4e71-a6bc-1d10bc87541b",
      "metadata": {
        "id": "4e749374-7b8b-4e71-a6bc-1d10bc87541b",
        "outputId": "5f180855-01ee-437f-b22c-2dc55a7ab064"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting statsmodels\n",
            "  Downloading statsmodels-0.14.2-cp310-cp310-win_amd64.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.22.3 in c:\\users\\arhat\\anaconda3\\envs\\tf\\lib\\site-packages (from statsmodels) (1.26.3)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in c:\\users\\arhat\\anaconda3\\envs\\tf\\lib\\site-packages (from statsmodels) (1.11.4)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in c:\\users\\arhat\\anaconda3\\envs\\tf\\lib\\site-packages (from statsmodels) (2.1.4)\n",
            "Collecting patsy>=0.5.6 (from statsmodels)\n",
            "  Downloading patsy-0.5.6-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in c:\\users\\arhat\\anaconda3\\envs\\tf\\lib\\site-packages (from statsmodels) (23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arhat\\anaconda3\\envs\\tf\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arhat\\anaconda3\\envs\\tf\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\arhat\\anaconda3\\envs\\tf\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2023.3)\n",
            "Requirement already satisfied: six in c:\\users\\arhat\\anaconda3\\envs\\tf\\lib\\site-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n",
            "Downloading statsmodels-0.14.2-cp310-cp310-win_amd64.whl (9.8 MB)\n",
            "   ---------------------------------------- 0.0/9.8 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.3/9.8 MB 5.2 MB/s eta 0:00:02\n",
            "   - -------------------------------------- 0.5/9.8 MB 6.0 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 0.7/9.8 MB 5.7 MB/s eta 0:00:02\n",
            "   ---- ----------------------------------- 1.0/9.8 MB 5.7 MB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 1.3/9.8 MB 5.7 MB/s eta 0:00:02\n",
            "   ------ --------------------------------- 1.6/9.8 MB 5.9 MB/s eta 0:00:02\n",
            "   ------- -------------------------------- 1.9/9.8 MB 6.0 MB/s eta 0:00:02\n",
            "   -------- ------------------------------- 2.1/9.8 MB 5.8 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 2.4/9.8 MB 5.9 MB/s eta 0:00:02\n",
            "   ---------- ----------------------------- 2.7/9.8 MB 5.9 MB/s eta 0:00:02\n",
            "   ------------ --------------------------- 3.0/9.8 MB 6.0 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 3.3/9.8 MB 6.0 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 3.6/9.8 MB 6.0 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 3.8/9.8 MB 6.0 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 4.2/9.8 MB 6.0 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 4.4/9.8 MB 6.0 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 4.7/9.8 MB 6.0 MB/s eta 0:00:01\n",
            "   -------------------- ------------------- 5.1/9.8 MB 6.1 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 5.4/9.8 MB 6.2 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 5.7/9.8 MB 6.2 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 5.9/9.8 MB 6.1 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 6.3/9.8 MB 6.2 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 6.6/9.8 MB 6.2 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 6.9/9.8 MB 6.2 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 7.2/9.8 MB 6.2 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 7.5/9.8 MB 6.2 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 7.8/9.8 MB 6.3 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 8.1/9.8 MB 6.3 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 8.5/9.8 MB 6.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 8.8/9.8 MB 6.4 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 9.1/9.8 MB 6.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 9.4/9.8 MB 6.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  9.8/9.8 MB 6.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  9.8/9.8 MB 6.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 9.8/9.8 MB 6.2 MB/s eta 0:00:00\n",
            "Downloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
            "   ---------------------------------------- 0.0/233.9 kB ? eta -:--:--\n",
            "   ---------------------------------------- 233.9/233.9 kB 7.2 MB/s eta 0:00:00\n",
            "Installing collected packages: patsy, statsmodels\n",
            "Successfully installed patsy-0.5.6 statsmodels-0.14.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install statsmodels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "734e34f1-4298-42ae-b359-071bf4415301",
      "metadata": {
        "id": "734e34f1-4298-42ae-b359-071bf4415301"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dfeab8c-be6f-4af7-9f65-87f68d2585ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dfeab8c-be6f-4af7-9f65-87f68d2585ea",
        "outputId": "f5b37614-8c2f-455a-e137-510a8bdda4d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (4900, 60, 1), y_train shape: (4900,)\n",
            "X_test shape: (1225, 60, 1), y_test shape: (1225,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load the stock data\n",
        "stock_symbol = 'AAPL'\n",
        "stock_data = yf.download(stock_symbol, start='2000-01-01')\n",
        "data = stock_data['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Scale the data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Prepare the dataset\n",
        "def create_dataset(dataset, time_step=1):\n",
        "    X, y = [], []\n",
        "    for i in range(len(dataset) - time_step):\n",
        "        a = dataset[i:(i + time_step), 0]\n",
        "        X.append(a)\n",
        "        y.append(dataset[i + time_step, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "time_step = 60\n",
        "X, y = create_dataset(scaled_data, time_step)\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70a72f5d-1177-481e-a940-7607963e04cb",
      "metadata": {
        "id": "70a72f5d-1177-481e-a940-7607963e04cb"
      },
      "source": [
        "### Auto-Regressor(ARIMA) architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c58116bb-af73-4891-a94f-94f894593a1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c58116bb-af73-4891-a94f-94f894593a1f",
        "outputId": "d5121005-4da6-45d7-8e15-6c50fd952cc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ARIMA 1-Day Forecast: [222.62306766]\n",
            "ARIMA 1-Week Forecast: [222.62306766 222.61364004 222.6163137  222.56147578 222.65567424\n",
            " 222.65363578 222.6522883 ]\n"
          ]
        }
      ],
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# Train ARIMA model\n",
        "arima_order = (5, 1, 0)  # Example order, should be tuned\n",
        "arima_model = ARIMA(data, order=arima_order)\n",
        "arima_result = arima_model.fit()\n",
        "\n",
        "# Predict the next 1 day and 1 week\n",
        "arima_forecast_1d = arima_result.forecast(steps=1)\n",
        "arima_forecast_1w = arima_result.forecast(steps=7)\n",
        "\n",
        "print(\"ARIMA 1-Day Forecast:\", arima_forecast_1d)\n",
        "print(\"ARIMA 1-Week Forecast:\", arima_forecast_1w)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83787fee-fc11-4924-a857-6286509d49e8",
      "metadata": {
        "id": "83787fee-fc11-4924-a857-6286509d49e8"
      },
      "source": [
        "### CNN-LSTM architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e1b477f-13d2-4ad4-9ce3-ed5252947f73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9e1b477f-13d2-4ad4-9ce3-ed5252947f73",
        "outputId": "f0ee58c7-34df-4dc4-d9f7-23c3a44fe188"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - loss: 0.0012 - val_loss: 0.0049\n",
            "Epoch 2/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 7.7534e-06 - val_loss: 0.0032\n",
            "Epoch 3/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 7.2491e-06 - val_loss: 0.0019\n",
            "Epoch 4/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 8.2295e-06 - val_loss: 8.6307e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 7.4003e-06 - val_loss: 5.1653e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 7.6365e-06 - val_loss: 4.7466e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 7.8660e-06 - val_loss: 4.3104e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 7.0770e-06 - val_loss: 4.1660e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 7.3254e-06 - val_loss: 3.8708e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 5.7087e-06 - val_loss: 3.7851e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 5.9661e-06 - val_loss: 5.2960e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 6.4188e-06 - val_loss: 4.2336e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 6.2813e-06 - val_loss: 6.2915e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.5937e-06 - val_loss: 3.8705e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 5.2335e-06 - val_loss: 4.5042e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 5.0987e-06 - val_loss: 2.5047e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 5.7546e-06 - val_loss: 2.6319e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 5.1234e-06 - val_loss: 4.2611e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.9341e-06 - val_loss: 2.2290e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.5191e-06 - val_loss: 2.0076e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 4.4261e-06 - val_loss: 2.2628e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 4.6269e-06 - val_loss: 2.7078e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.3152e-06 - val_loss: 1.8526e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.9201e-06 - val_loss: 1.9613e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.9255e-06 - val_loss: 4.4946e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.6541e-06 - val_loss: 3.6487e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 5.1055e-06 - val_loss: 4.9984e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 4.4209e-06 - val_loss: 5.9699e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 4.1126e-06 - val_loss: 8.1765e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 3.8013e-06 - val_loss: 5.6041e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.4987e-06 - val_loss: 6.3397e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.7341e-06 - val_loss: 0.0014\n",
            "Epoch 33/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.0238e-06 - val_loss: 9.5429e-04\n",
            "Epoch 34/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - loss: 3.7344e-06 - val_loss: 5.6843e-04\n",
            "Epoch 35/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.0265e-06 - val_loss: 5.7850e-04\n",
            "Epoch 36/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 3.6743e-06 - val_loss: 8.1931e-04\n",
            "Epoch 37/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.4459e-06 - val_loss: 0.0022\n",
            "Epoch 38/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.2820e-06 - val_loss: 0.0016\n",
            "Epoch 39/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 3.0829e-06 - val_loss: 0.0012\n",
            "Epoch 40/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 3.9190e-06 - val_loss: 0.0019\n",
            "Epoch 41/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 3.7955e-06 - val_loss: 9.6733e-04\n",
            "Epoch 42/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 3.1292e-06 - val_loss: 0.0013\n",
            "Epoch 43/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 4.3670e-06 - val_loss: 0.0019\n",
            "Epoch 44/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 4.9871e-06 - val_loss: 0.0013\n",
            "Epoch 45/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 2.8123e-06 - val_loss: 6.7178e-04\n",
            "Epoch 46/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 2.9795e-06 - val_loss: 9.6101e-04\n",
            "Epoch 47/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 2.9998e-06 - val_loss: 4.5029e-04\n",
            "Epoch 48/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - loss: 4.0526e-06 - val_loss: 0.0013\n",
            "Epoch 49/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 3.3906e-06 - val_loss: 0.0017\n",
            "Epoch 50/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 3.4396e-06 - val_loss: 0.0017\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "CNN-LSTM 1-Day Forecast: 197.6275\n",
            "CNN-LSTM 1-Week Forecast: [197.6275, 202.07445, 198.06718, 197.21194, 196.47119, 194.9531, 195.81776]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, LSTM, Dense\n",
        "\n",
        "# Define the CNN-LSTM model\n",
        "cnn_lstm_model = Sequential([\n",
        "    Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(time_step, 1)),\n",
        "    LSTM(50, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "cnn_lstm_model.compile(optimizer='adam', loss='mse')\n",
        "cnn_lstm_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predict the next 1 day and 1 week\n",
        "cnn_lstm_forecast_1d = cnn_lstm_model.predict(X_test[-1].reshape(1, time_step, 1))\n",
        "cnn_lstm_forecast_1w = [cnn_lstm_model.predict(X_test[-(i + 1)].reshape(1, time_step, 1)) for i in range(7)]\n",
        "\n",
        "# Inverse transform the predictions to get actual stock prices\n",
        "cnn_lstm_forecast_1d = scaler.inverse_transform(cnn_lstm_forecast_1d)[0][0]\n",
        "cnn_lstm_forecast_1w = [scaler.inverse_transform(f)[0][0] for f in cnn_lstm_forecast_1w]\n",
        "\n",
        "print(\"CNN-LSTM 1-Day Forecast:\", cnn_lstm_forecast_1d)\n",
        "print(\"CNN-LSTM 1-Week Forecast:\", cnn_lstm_forecast_1w)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mCYKOyJe5Cr-",
      "metadata": {
        "id": "mCYKOyJe5Cr-"
      },
      "source": [
        "### Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0IYC3e8tA5zn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IYC3e8tA5zn",
        "outputId": "916521d6-8530-4eb4-9415-147b51ddc151"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 67ms/step - loss: 0.0058 - val_loss: 0.3294\n",
            "Epoch 2/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.3229\n",
            "Epoch 3/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0041 - val_loss: 0.3250\n",
            "Epoch 4/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0043 - val_loss: 0.3275\n",
            "Epoch 5/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3288\n",
            "Epoch 6/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0044 - val_loss: 0.3255\n",
            "Epoch 7/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3292\n",
            "Epoch 8/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.3304\n",
            "Epoch 9/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0043 - val_loss: 0.3274\n",
            "Epoch 10/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.3324\n",
            "Epoch 11/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3260\n",
            "Epoch 12/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3280\n",
            "Epoch 13/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.3242\n",
            "Epoch 14/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3288\n",
            "Epoch 15/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3315\n",
            "Epoch 16/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.3284\n",
            "Epoch 17/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.3279\n",
            "Epoch 18/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3275\n",
            "Epoch 19/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.3295\n",
            "Epoch 20/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3276\n",
            "Epoch 21/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0041 - val_loss: 0.3281\n",
            "Epoch 22/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0042 - val_loss: 0.3262\n",
            "Epoch 23/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3296\n",
            "Epoch 24/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.3292\n",
            "Epoch 25/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3267\n",
            "Epoch 26/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.3271\n",
            "Epoch 27/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0041 - val_loss: 0.3290\n",
            "Epoch 28/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0041 - val_loss: 0.3264\n",
            "Epoch 29/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.3279\n",
            "Epoch 30/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3298\n",
            "Epoch 31/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0041 - val_loss: 0.3225\n",
            "Epoch 32/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3248\n",
            "Epoch 33/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.3250\n",
            "Epoch 34/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3273\n",
            "Epoch 35/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3286\n",
            "Epoch 36/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3244\n",
            "Epoch 37/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.3269\n",
            "Epoch 38/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3298\n",
            "Epoch 39/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0043 - val_loss: 0.3240\n",
            "Epoch 40/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0042 - val_loss: 0.3257\n",
            "Epoch 41/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.3286\n",
            "Epoch 42/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.3257\n",
            "Epoch 43/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3283\n",
            "Epoch 44/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0041 - val_loss: 0.3252\n",
            "Epoch 45/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3238\n",
            "Epoch 46/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0041 - val_loss: 0.3246\n",
            "Epoch 47/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3273\n",
            "Epoch 48/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.3256\n",
            "Epoch 49/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.3286\n",
            "Epoch 50/50\n",
            "\u001b[1m154/154\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0041 - val_loss: 0.3253\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 868ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Transformer 1-Day Forecast: 14.684911\n",
            "Transformer 1-Week Forecast: [14.684911 14.684911 14.684911 14.684911 14.684911 14.684911 14.684911]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout, MultiHeadAttention, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "\n",
        "# Define TransformerBlock layer\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),\n",
        "            Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Define Transformer model\n",
        "def create_transformer_model(input_shape, num_heads, ff_dim, num_transformer_blocks, rate=0.1):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = TransformerBlock(input_shape[1], num_heads, ff_dim, rate)(x)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dropout(rate)(x)\n",
        "    x = Dense(20, activation=\"relu\")(x)\n",
        "    x = Dropout(rate)(x)\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Create and compile the Transformer model\n",
        "transformer_model = create_transformer_model((time_step, 1), num_heads=2, ff_dim=32, num_transformer_blocks=2, rate=0.1)\n",
        "transformer_model.compile(optimizer='adam', loss='mse')\n",
        "transformer_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predict the next 1 day and 1 week\n",
        "transformer_forecast_1d = transformer_model.predict(X_test[-1].reshape(1, time_step, 1))\n",
        "transformer_forecast_1w = []\n",
        "\n",
        "# Rolling prediction for the next 1 week\n",
        "current_input = X_test[-1].reshape(1, time_step, 1)\n",
        "for _ in range(7):\n",
        "    pred = transformer_model.predict(current_input)\n",
        "    transformer_forecast_1w.append(pred[0, 0])\n",
        "    # Reshape pred to match the shape of current_input\n",
        "    pred = pred.reshape(1, 1, 1)\n",
        "    # Update the input for the next prediction\n",
        "    current_input = np.append(current_input[:, 1:, :], pred, axis=1)\n",
        "\n",
        "# Inverse transform the predictions to get actual stock prices\n",
        "transformer_forecast_1d = scaler.inverse_transform(transformer_forecast_1d)[0][0]\n",
        "transformer_forecast_1w = scaler.inverse_transform(np.array(transformer_forecast_1w).reshape(-1, 1)).flatten()\n",
        "\n",
        "print(\"Transformer 1-Day Forecast:\", transformer_forecast_1d)\n",
        "print(\"Transformer 1-Week Forecast:\", transformer_forecast_1w)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce32d158-ef83-495d-bd1c-205fd2f5788a",
      "metadata": {
        "id": "ce32d158-ef83-495d-bd1c-205fd2f5788a"
      },
      "source": [
        "### Result and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aabf25c-f0f5-42cb-982d-fd09eff17ec2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aabf25c-f0f5-42cb-982d-fd09eff17ec2",
        "outputId": "79ae6938-50fe-4e41-946e-8633738b22e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ARIMA 1-Day Forecast: [222.62306766]\n",
            "ARIMA 1-Week Forecast: [222.62306766 222.61364004 222.6163137  222.56147578 222.65567424\n",
            " 222.65363578 222.6522883 ]\n",
            "CNN-LSTM 1-Day Forecast: 197.6275\n",
            "CNN-LSTM 1-Week Forecast: [197.6275, 202.07445, 198.06718, 197.21194, 196.47119, 194.9531, 195.81776]\n",
            "Transformer 1-Day Forecast: 14.684911\n",
            "Transformer 1-Week Forecast: [14.684911 14.684911 14.684911 14.684911 14.684911 14.684911 14.684911]\n"
          ]
        }
      ],
      "source": [
        "# Print forecasts for each model\n",
        "print(\"ARIMA 1-Day Forecast:\", arima_forecast_1d)\n",
        "print(\"ARIMA 1-Week Forecast:\", arima_forecast_1w)\n",
        "\n",
        "print(\"CNN-LSTM 1-Day Forecast:\", cnn_lstm_forecast_1d)\n",
        "print(\"CNN-LSTM 1-Week Forecast:\", cnn_lstm_forecast_1w)\n",
        "\n",
        "print(\"Transformer 1-Day Forecast:\", transformer_forecast_1d)\n",
        "print(\"Transformer 1-Week Forecast:\", transformer_forecast_1w)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bb97a36-6c96-4aed-8938-5374b1e1efeb",
      "metadata": {
        "id": "4bb97a36-6c96-4aed-8938-5374b1e1efeb"
      },
      "source": [
        "## Fine-Tuning Hyperparameters to reduce the loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47175540-b69c-4eb1-a387-4b6217c0b8f2",
      "metadata": {
        "id": "47175540-b69c-4eb1-a387-4b6217c0b8f2"
      },
      "source": [
        "### 1) Auto Regressor architecture finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yUzw0hotjN1B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUzw0hotjN1B",
        "outputId": "a89e455f-5b4f-422b-a037-c3e5a6524e89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pmdarima\n",
            "  Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.4.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (2.1.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.13.1)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (0.14.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (2.0.7)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (71.0.4)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (24.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->pmdarima) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->pmdarima) (0.5.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels>=0.13.2->pmdarima) (1.16.0)\n",
            "Downloading pmdarima-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pmdarima\n",
            "Successfully installed pmdarima-2.0.4\n"
          ]
        }
      ],
      "source": [
        "pip install pmdarima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fjpV288g_Tq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fjpV288g_Tq",
        "outputId": "76ef0583-c669-4a35-df47-a760f1f04e53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing stepwise search to minimize aic\n",
            " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=3234.983, Time=1.54 sec\n",
            " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=3236.884, Time=0.60 sec\n",
            " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=3236.876, Time=1.01 sec\n",
            " ARIMA(0,1,0)(0,0,0)[0]             : AIC=3237.549, Time=0.27 sec\n",
            " ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=3238.029, Time=3.88 sec\n",
            "\n",
            "Best model:  ARIMA(0,1,0)(0,0,0)[0] intercept\n",
            "Total fit time: 7.326 seconds\n",
            "                               SARIMAX Results                                \n",
            "==============================================================================\n",
            "Dep. Variable:                      y   No. Observations:                 4948\n",
            "Model:               SARIMAX(0, 1, 0)   Log Likelihood               -1615.491\n",
            "Date:                Sat, 03 Aug 2024   AIC                           3234.983\n",
            "Time:                        09:25:06   BIC                           3247.996\n",
            "Sample:                             0   HQIC                          3239.546\n",
            "                               - 4948                                         \n",
            "Covariance Type:                  opg                                         \n",
            "==============================================================================\n",
            "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "intercept      0.0102      0.005      2.113      0.035       0.001       0.020\n",
            "sigma2         0.1125      0.001    164.355      0.000       0.111       0.114\n",
            "===================================================================================\n",
            "Ljung-Box (L1) (Q):                   0.10   Jarque-Bera (JB):             85774.75\n",
            "Prob(Q):                              0.75   Prob(JB):                         0.00\n",
            "Heteroskedasticity (H):             248.16   Skew:                            -0.71\n",
            "Prob(H) (two-sided):                  0.00   Kurtosis:                        23.35\n",
            "===================================================================================\n",
            "\n",
            "Warnings:\n",
            "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n",
            "Auto ARIMA 1-Day Forecast: [51.4351924]\n",
            "Auto ARIMA 1-Week Forecast: [51.4351924  51.44538555 51.45557871 51.46577187 51.47596503 51.48615819\n",
            " 51.49635135]\n",
            "Test MSE: 8418.792906008388\n"
          ]
        }
      ],
      "source": [
        "import pmdarima as pm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "\n",
        "# Load the stock data\n",
        "stock_symbol = 'AAPL'\n",
        "stock_data = yf.download(stock_symbol, start='2000-01-01')\n",
        "data = stock_data['Close'].values\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_size = int(len(data) * 0.8)\n",
        "train_data, test_data = data[:train_size], data[train_size:]\n",
        "\n",
        "# Fit the ARIMA model with automatic hyperparameter tuning\n",
        "model = pm.auto_arima(\n",
        "    train_data,\n",
        "    start_p=0,\n",
        "    start_q=0,\n",
        "    max_p=6,\n",
        "    max_q=6,\n",
        "    seasonal=False,\n",
        "    trace=True,\n",
        "    error_action='ignore',\n",
        "    suppress_warnings=True,\n",
        "    stepwise=True\n",
        ")\n",
        "\n",
        "# Print the model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Forecast the next 1 day and 1 week\n",
        "forecast_1d = model.predict(n_periods=1)\n",
        "forecast_1w = model.predict(n_periods=7)\n",
        "\n",
        "print(\"Auto ARIMA 1-Day Forecast:\", forecast_1d)\n",
        "print(\"Auto ARIMA 1-Week Forecast:\", forecast_1w)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "predictions = model.predict(n_periods=len(test_data))\n",
        "mse = mean_squared_error(test_data, predictions)\n",
        "print(f\"Test MSE: {mse}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kpC8UWmN1NgN",
      "metadata": {
        "id": "kpC8UWmN1NgN"
      },
      "source": [
        "### 2) CNN - LSTM architecture finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "g-lMEWO7Tokv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-lMEWO7Tokv",
        "outputId": "29388121-26f5-4ae2-972e-02dc41561619"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model with filters=128, kernel_size=3, lstm_units=100, learning_rate=0.001, batch_size=16\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step\n",
            "Model MSE: 0.00045404891630825743\n",
            "Training model with filters=64, kernel_size=3, lstm_units=100, learning_rate=0.001, batch_size=64\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
            "Model MSE: 0.0009784592928676252\n",
            "Training model with filters=64, kernel_size=4, lstm_units=50, learning_rate=0.001, batch_size=64\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n",
            "Model MSE: 0.002165623752749002\n",
            "Training model with filters=32, kernel_size=2, lstm_units=100, learning_rate=0.01, batch_size=32\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
            "Model MSE: 53204.22666318027\n",
            "Training model with filters=32, kernel_size=2, lstm_units=50, learning_rate=0.001, batch_size=32\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step\n",
            "Model MSE: 0.00014497386190118832\n",
            "Training model with filters=64, kernel_size=3, lstm_units=100, learning_rate=0.01, batch_size=64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
            "Model MSE: 0.13731153311482888\n",
            "Training model with filters=128, kernel_size=2, lstm_units=150, learning_rate=0.001, batch_size=32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step\n",
            "Model MSE: 0.0002202809265694602\n",
            "Training model with filters=64, kernel_size=4, lstm_units=100, learning_rate=0.001, batch_size=32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
            "Model MSE: 0.0037175917258935247\n",
            "Training model with filters=32, kernel_size=4, lstm_units=100, learning_rate=0.01, batch_size=16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
            "Model MSE: 0.00028741596078182586\n",
            "Training model with filters=32, kernel_size=2, lstm_units=150, learning_rate=0.001, batch_size=64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
            "Model MSE: 0.0010140587295140524\n",
            "Best CNN-LSTM params: {'filters': 32, 'kernel_size': 2, 'lstm_units': 50, 'learning_rate': 0.001, 'batch_size': 32}\n",
            "Best CNN-LSTM MSE: 0.00014497386190118832\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, LSTM, Dense\n",
        "import random\n",
        "\n",
        "# Load the stock data\n",
        "stock_symbol = 'AAPL'\n",
        "stock_data = yf.download(stock_symbol, start='2000-01-01')\n",
        "data = stock_data['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Scale the data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Prepare the dataset\n",
        "def create_dataset(dataset, time_step=1):\n",
        "    X, y = [], []\n",
        "    for i in range(len(dataset) - time_step):\n",
        "        X.append(dataset[i:(i + time_step), 0])\n",
        "        y.append(dataset[i + time_step, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "time_step = 60\n",
        "X, y = create_dataset(scaled_data, time_step)\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Define the CNN-LSTM model\n",
        "def create_cnn_lstm_model(filters=64, kernel_size=2, lstm_units=50, learning_rate=0.001):\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', input_shape=(time_step, 1)),\n",
        "        LSTM(lstm_units, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "    return model\n",
        "\n",
        "# Hyperparameter space\n",
        "filter_space = [32, 64, 128]\n",
        "kernel_size_space = [2, 3, 4]\n",
        "lstm_units_space = [50, 100, 150]\n",
        "learning_rate_space = [0.001, 0.01]\n",
        "batch_size_space = [16, 32, 64]\n",
        "epochs = 50\n",
        "\n",
        "# Random search\n",
        "n_iter = 10\n",
        "best_mse = float(\"inf\")\n",
        "best_params = {}\n",
        "\n",
        "for _ in range(n_iter):\n",
        "    filters = random.choice(filter_space)\n",
        "    kernel_size = random.choice(kernel_size_space)\n",
        "    lstm_units = random.choice(lstm_units_space)\n",
        "    learning_rate = random.choice(learning_rate_space)\n",
        "    batch_size = random.choice(batch_size_space)\n",
        "\n",
        "    print(f\"Training model with filters={filters}, kernel_size={kernel_size}, lstm_units={lstm_units}, learning_rate={learning_rate}, batch_size={batch_size}\")\n",
        "\n",
        "    model = create_cnn_lstm_model(filters, kernel_size, lstm_units, learning_rate)\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=0)\n",
        "\n",
        "    # Evaluate the model\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"Model MSE: {mse}\")\n",
        "\n",
        "    if mse < best_mse:\n",
        "        best_mse = mse\n",
        "        best_params = {\n",
        "            'filters': filters,\n",
        "            'kernel_size': kernel_size,\n",
        "            'lstm_units': lstm_units,\n",
        "            'learning_rate': learning_rate,\n",
        "            'batch_size': batch_size\n",
        "        }\n",
        "\n",
        "print(\"Best CNN-LSTM params:\", best_params)\n",
        "print(\"Best CNN-LSTM MSE:\", best_mse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Transformer architecture finetuning"
      ],
      "metadata": {
        "id": "wgg1PZ9nP_uI"
      },
      "id": "wgg1PZ9nP_uI"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout, MultiHeadAttention, GlobalAveragePooling1D\n",
        "import random\n",
        "\n",
        "# Load the stock data\n",
        "stock_symbol = 'AAPL'\n",
        "stock_data = yf.download(stock_symbol, start='2000-01-01')\n",
        "data = stock_data['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Scale the data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Prepare the dataset\n",
        "def create_dataset(dataset, time_step=1):\n",
        "    X, y = [], []\n",
        "    for i in range(len(dataset) - time_step):\n",
        "        X.append(dataset[i:(i + time_step), 0])\n",
        "        y.append(dataset[i + time_step, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "time_step = 60\n",
        "X, y = create_dataset(scaled_data, time_step)\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)]\n",
        "        )\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "def create_transformer_model(input_shape, num_heads, ff_dim, num_transformer_blocks, rate=0.1):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = TransformerBlock(input_shape[1], num_heads, ff_dim, rate)(x)\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    x = Dropout(rate)(x)\n",
        "    x = Dense(20, activation=\"relu\")(x)\n",
        "    x = Dropout(rate)(x)\n",
        "    outputs = Dense(1)(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Hyperparameter space\n",
        "num_heads_space = [2, 4, 6, 8]\n",
        "ff_dim_space = [32, 64, 128]\n",
        "num_transformer_blocks_space = [1, 2, 3]\n",
        "dropout_rate_space = [0.1, 0.2, 0.3]\n",
        "learning_rate_space = [0.001, 0.01]\n",
        "batch_size_space = [16, 32, 64]\n",
        "epochs = 50\n",
        "\n",
        "# Random search\n",
        "n_iter = 20\n",
        "best_mse = float(\"inf\")\n",
        "best_params = {}\n",
        "\n",
        "for _ in range(n_iter):\n",
        "    num_heads = random.choice(num_heads_space)\n",
        "    ff_dim = random.choice(ff_dim_space)\n",
        "    num_transformer_blocks = random.choice(num_transformer_blocks_space)\n",
        "    dropout_rate = random.choice(dropout_rate_space)\n",
        "    learning_rate = random.choice(learning_rate_space)\n",
        "    batch_size = random.choice(batch_size_space)\n",
        "\n",
        "    print(f\"Training model with num_heads={num_heads}, ff_dim={ff_dim}, num_transformer_blocks={num_transformer_blocks}, dropout_rate={dropout_rate}, learning_rate={learning_rate}, batch_size={batch_size}\")\n",
        "\n",
        "    model = create_transformer_model((time_step, 1), num_heads, ff_dim, num_transformer_blocks, dropout_rate)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), verbose=0)\n",
        "\n",
        "    # Evaluate the model\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    print(f\"Model MSE: {mse}\")\n",
        "\n",
        "    if mse < best_mse:\n",
        "        best_mse = mse\n",
        "        best_params = {\n",
        "            'num_heads': num_heads,\n",
        "            'ff_dim': ff_dim,\n",
        "            'num_transformer_blocks': num_transformer_blocks,\n",
        "            'dropout_rate': dropout_rate,\n",
        "            'learning_rate': learning_rate,\n",
        "            'batch_size': batch_size\n",
        "        }\n",
        "\n",
        "print(\"Best Transformer params:\", best_params)\n",
        "print(\"Best Transformer MSE:\", best_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGAq6DTtRz0k",
        "outputId": "0c93311c-f355-4962-a7b7-3967d5fe58e2"
      },
      "id": "jGAq6DTtRz0k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with num_heads=6, ff_dim=32, num_transformer_blocks=3, dropout_rate=0.2, learning_rate=0.01, batch_size=16\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step\n",
            "Model MSE: 0.32335426839520254\n",
            "Training model with num_heads=8, ff_dim=32, num_transformer_blocks=1, dropout_rate=0.2, learning_rate=0.01, batch_size=16\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
            "Model MSE: 0.3241814895689793\n",
            "Training model with num_heads=4, ff_dim=64, num_transformer_blocks=3, dropout_rate=0.3, learning_rate=0.001, batch_size=16\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
            "Model MSE: 0.32016331441607493\n",
            "Training model with num_heads=6, ff_dim=32, num_transformer_blocks=2, dropout_rate=0.1, learning_rate=0.001, batch_size=64\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
            "Model MSE: 0.3253481064813118\n",
            "Training model with num_heads=4, ff_dim=32, num_transformer_blocks=1, dropout_rate=0.1, learning_rate=0.001, batch_size=32\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
            "Model MSE: 0.32793387511086997\n",
            "Training model with num_heads=8, ff_dim=32, num_transformer_blocks=3, dropout_rate=0.2, learning_rate=0.01, batch_size=64\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step\n",
            "Model MSE: 0.3212916495320784\n",
            "Training model with num_heads=2, ff_dim=128, num_transformer_blocks=1, dropout_rate=0.2, learning_rate=0.001, batch_size=32\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
            "Model MSE: 0.32858063444860736\n",
            "Training model with num_heads=4, ff_dim=128, num_transformer_blocks=3, dropout_rate=0.1, learning_rate=0.001, batch_size=32\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Model MSE: 0.32478661909422457\n",
            "Training model with num_heads=2, ff_dim=128, num_transformer_blocks=3, dropout_rate=0.2, learning_rate=0.01, batch_size=64\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
            "Model MSE: 0.325666205694036\n",
            "Training model with num_heads=6, ff_dim=64, num_transformer_blocks=3, dropout_rate=0.3, learning_rate=0.01, batch_size=16\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
            "Model MSE: 0.34046153413508023\n",
            "Training model with num_heads=4, ff_dim=128, num_transformer_blocks=1, dropout_rate=0.1, learning_rate=0.01, batch_size=64\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
            "Model MSE: 0.3225904066843677\n",
            "Training model with num_heads=8, ff_dim=128, num_transformer_blocks=1, dropout_rate=0.2, learning_rate=0.01, batch_size=16\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step\n",
            "Model MSE: 0.32847968981132053\n",
            "Training model with num_heads=6, ff_dim=64, num_transformer_blocks=2, dropout_rate=0.3, learning_rate=0.01, batch_size=16\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
            "Model MSE: 0.3182528768230804\n",
            "Training model with num_heads=8, ff_dim=128, num_transformer_blocks=1, dropout_rate=0.2, learning_rate=0.001, batch_size=64\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step\n",
            "Model MSE: 0.3280164166342499\n",
            "Training model with num_heads=2, ff_dim=64, num_transformer_blocks=3, dropout_rate=0.2, learning_rate=0.01, batch_size=32\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
            "Model MSE: 0.3438823941601111\n",
            "Training model with num_heads=4, ff_dim=32, num_transformer_blocks=3, dropout_rate=0.1, learning_rate=0.01, batch_size=64\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "Model MSE: 0.3206307388065756\n",
            "Training model with num_heads=4, ff_dim=32, num_transformer_blocks=2, dropout_rate=0.2, learning_rate=0.001, batch_size=16\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n",
            "Model MSE: 0.3275862741260059\n",
            "Training model with num_heads=6, ff_dim=32, num_transformer_blocks=2, dropout_rate=0.3, learning_rate=0.01, batch_size=32\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n",
            "Model MSE: 0.33156651004979043\n",
            "Training model with num_heads=2, ff_dim=64, num_transformer_blocks=2, dropout_rate=0.1, learning_rate=0.01, batch_size=64\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
            "Model MSE: 0.3313325393602382\n",
            "Training model with num_heads=2, ff_dim=64, num_transformer_blocks=2, dropout_rate=0.2, learning_rate=0.001, batch_size=32\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
            "Model MSE: 0.3226727398929859\n",
            "Best Transformer params: {'num_heads': 6, 'ff_dim': 64, 'num_transformer_blocks': 2, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'batch_size': 16}\n",
            "Best Transformer MSE: 0.3182528768230804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y-UvzztHVb2q"
      },
      "id": "Y-UvzztHVb2q",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}